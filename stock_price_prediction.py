# -*- coding: utf-8 -*-
"""Stock Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbwXQtB6hdtb8YAA1Hvo6CR6TyaMg7Kg
"""

import  numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf

start = '2013-01-01'
end = '2024-08-24'
stock = 'ZOMATO.NS'

data = yf.download(stock, start, end)

data.reset_index(inplace =True)

data

# Calculate the 100-day moving average
moving_avg_100days = close_prices.rolling(window=100).mean()

# Plot the data
plt.figure(figsize=(8, 6))
plt.plot(moving_avg_100days, 'r', label='100-Day Moving Average')  # 100-day moving average in red
plt.plot(close_prices, 'g', label='Close Price')  # Close price in green
plt.xlabel('Time')
plt.ylabel('Price')
plt.title('Stock Price and 100-Day Moving Average')
plt.legend()
plt.show()

moving_avg_200days = data.Close.rolling(200).mean()

plt.figure(figsize=(8, 6))
plt.plot(moving_avg_200days, 'r', label = 'moving avg')
plt.plot(data.Close, 'g', label='Close Price')
plt.xlabel('Time')
plt.ylabel('Price')
plt.title('Stock Price and 200-Day Moving Average')
plt.legend()
plt.show()

data.dropna(inplace=True)

data_train = pd.DataFrame(data.Close[0: int(len(data)*0.80)] )
data_test = pd.DataFrame(data=data.Close[int(len(data)*0.80): len(data)])

data_train.shape[0]

data_test.shape[0]

from sklearn.preprocessing import MinMaxScaler
Scaler = MinMaxScaler(feature_range=(0,1))

data_train_scale = Scaler.fit_transform(data_train)

x = []
y = []

for i in range(100, data_train_scale.shape[0]):
  x.append(data_train_scale[i-100:i])
  y.append(data_train_scale[i,0])

x,y = np.array(x), np.array(y)

from keras.layers import Dense, Dropout, LSTM
from keras.models import Sequential
from tensorflow.keras.optimizers import Adam

model = Sequential()

# Add the first LSTM layer with input shape
#model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x.shape[1], 1)))
model.add(Dropout(0.2))

# Add the second LSTM layer
model.add(LSTM(units=60, activation='relu', return_sequences=True))
model.add(Dropout(0.3))

# Add the third LSTM layer
model.add(LSTM(units=80, activation='relu', return_sequences=True))
model.add(Dropout(0.4))

# Add the fourth LSTM layer
model.add(LSTM(units=120, activation='relu'))
model.add(Dropout(0.5))

# Add the output layer
model.add(Dense(units=1))

model.compile(optimizer= 'adam', loss = 'mean_squared_error')

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense

# Assuming `data` is your original DataFrame containing the 'Close' prices
data = pd.DataFrame({
    'Close': np.random.rand(150)  # Example with 150 random data points
})

# Scale the 'Close' prices between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
data_test_scale = scaler.fit_transform(data['Close'].values.reshape(-1, 1))

# Create sequences and targets
x = []
y = []

for i in range(100, data_test_scale.shape[0]):
    x.append(data_test_scale[i-100:i])
    y.append(data_test_scale[i, 0])

x = np.array(x)
y = np.array(y)

# Check shapes
print(f"x shape: {x.shape}")  # Expected: (num_samples, 100, 1)
print(f"y shape: {y.shape}")  # Expected: (num_samples,)

# Initialize the model
model = Sequential()
model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=60, activation='relu', return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(units=80, activation='relu', return_sequences=False))
model.add(Dropout(0.4))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Fit the model
model.fit(x, y, epochs=50, batch_size=32, verbose=1)

pas_100_days = data_train.tail(100)

data_test = pd.concat([pas_100_days, data_test], ignore_index= True)

data_test_scale = Scaler.fit_transform(data_test)

x = []
y = []

# Generate input sequences and target values
for i in range(100, data_test_scale.shape[0]):
    x.append(data_test_scale[i-100:i])
    y.append(data_test_scale[i, 0])

# Convert lists to NumPy arrays after the loop
x = np.array(x)
y = np.array(y)

# Make predictions
y_predict = model.predict(x)

# Check the shape and first few predictions
print(f"y_predict shape: {y_predict.shape}")
print(y_predict[:10])  # Print first 5 predictions

y_predict

scale =10/scaler.scale_

y_predict = y_predict*scale

y = y*scale

plt.figure(figsize=(10,8))
plt.plot(y_predict, 'r', label = 'Predicted Price')
plt.plot(y, 'g', label = 'Original Price')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

# implement cludtring on a given data set

# Import necessary libraries
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np # importing numpy


# Step 1: Load your dataset (replace 'your_data.csv' with your actual file)
# Example dataset could have columns like 'Feature1', 'Feature2', etc.
df = pd.read_csv('/content/totalearning.csv')

# Step 2: Preprocess the dataset
# Replace 'Actual_Feature1', 'Actual_Feature2' with the actual column names from your dataset
X = df[['Total Earning']]  # Use actual column names

# Handle NaN values by replacing them with the mean of the column
X['Total Earning'] = X['Total Earning'].fillna(X['Total Earning'].mean()) # Replacing NaN values with the mean

# Normalize the data for better performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)  # Specify the number of clusters
kmeans.fit(X_scaled)

# Add the cluster labels to the dataset
df['Cluster'] = kmeans.labels_

# Step 4: Visualize the clusters
plt.figure(figsize=(8, 6))
# Replace 'Actual_Feature1', 'Actual_Feature2' with actual column names for plotting
plt.scatter(df['Total Earning'], df['Total Order'], c=df['Cluster'], cmap='viridis')
plt.xlabel('Total Earning')  # Update x-axis label
plt.ylabel('Total Order')  # Update y-axis label
plt.title('K-means Clustering')
plt.show()

# Optional: Display cluster centers
print("Cluster Centers:")
print(scaler.inverse_transform(kmeans.cluster_centers_))  # Display original scale

